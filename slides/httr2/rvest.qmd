---
engine: knitr
title: Scrape data from web pages 
---

# Ô∏è‚úÖ Learning objectives

::: nonincremental
-   Decide whether to scrape data from a web page.
-   Scrape tables from web pages.
-   Scrape more complex data structures from web pages.
-   Scrape data from websites that require you to log in.
-   Scrape content that requires interaction.
:::

::: notes
-   "Scraping" a web page means programmatically processing the web page to extract data in a usable format.
-   Can be a single page or several.
:::

# Deciding whether to scrape data

## Do I need to scrape this data?

-   Try {[datapasta](https://cran.r-project.org/package=datapasta)} üì¶
    -   RStudio Addins
-   If it's one time & over-complicated, consider other copy/paste strategies
-   Only scrape what you need
-   Look for an API!

::: notes
-   Even consider manual copy/paste one field at a time if it's not a TON of data.
-   More on looking for APIs in chapter 9 "Find APIs"
:::

## Can I legally scrape this data?

-   ‚úÖ Personal use or nonprofit education usually ok
-   ‚ö†Ô∏è Personally Identifiable Information (PII)
-   ‚ö†Ô∏è Legal disclaimers (but may be over-protective)
-   USA:
    -   Can't copyright facts,
    -   *CAN* copyright collections of facts in some cases (creative)
    -   [TidyTuesday 2023-08-29: Fair Use](https://tidytues.day/2023/2023-08-29)
-   Other places:
    -   Sometimes stricter ([EU](https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:31996L0009:EN:HTML))
    -   Sometimes more lax

::: notes
-   If it's important, consult a lawyer.
-   PII: Or even user-generated information in some cases
-   This is why recipe blogs have stories about the recipes.
-   That TidyTuesday dataset exists because I was writing these notes.
-   EU provides more protections for database creators and for user-generated data
:::

## Should I scrape this data?

`robots.txt`

-   User-agent: * = everybody
-   Search for name(s) of package(s) 
-   Search for specific pages
-   Check root of site (`/`) and your particular subfolder 
-   These aren't (necessarily) legally binding

::: notes
-   Again, robots might be OVER protective, but watch for notes on how to use it
-   Don't hit a site you like a billion times in fast succession, you could "use up" their monthly bandwidth, for example
-   Your actions COULD cost them money, so don't be a jerk!
:::

# Scraping tables

## Example 1: Single table

(screenshot of table on page)

## Scraping a table

(code demo of scraping a table)

## Example 2: Multiple tables

(screenshot of tables on page)

## Choosing a table

(code demo of scraping one of many tables)

## Scraping multiple tables

(purrr)

# Scraping non-tabular data

## Example 3: Non-tabular data

(screenshot of page with non-tabular data, possibly CSS selector rules)

## Using SelectorGadget

(record clicks? also show code where it goes)

## Using CSS selectors

# Scraping interactive web pages

## Example 4: Login

## Scraping data behind a login

## Example 5: Dynamic data

## Scraping data that requires interaction
