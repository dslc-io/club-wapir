---
engine: knitr
title: Scrape data from web pages 
---

# Ô∏è‚úÖ Learning objectives

::: nonincremental
-   Decide whether to scrape data from a web page.
-   Scrape tables from web pages.
-   Scrape more complex data structures from web pages.
-   Scrape data from websites that require you to log in.
-   Scrape content that requires interaction.
:::

```{r}
#| label: rvest-packages-used
#| eval: true
library(polite)
library(rvest)
library(xml2)
```

::: notes
-   "Scraping" a web page means programmatically processing the web page to extract data in a usable format.
-   Can be a single page or several.
-   Point out that everything after the very beginning is the same as parsing HTML responses from APIs, because web pages are just GET requests
:::

# Deciding whether to scrape data

## Do I need to scrape this data?

-   Try {[datapasta](https://cran.r-project.org/package=datapasta)} üì¶
    -   RStudio Addins
-   If it's one time & over-complicated, consider other copy/paste strategies
-   Only scrape what you need
-   Look for an API!

::: notes
-   Even consider manual copy/paste one field at a time if it's not a TON of data.
-   We'll see one place to look for an API on the next slide.
-   More on looking for APIs in chapter 9 "Find APIs"
:::

## Can I legally scrape this data?

-   ‚úÖ Personal use or nonprofit education usually ok
-   ‚ö†Ô∏è Personally Identifiable Information (PII)
-   ‚ö†Ô∏è Legal disclaimers (but may be over-protective)
-   USA:
    -   Can't copyright facts,
    -   *CAN* copyright collections of facts in some cases (creative)
    -   [TidyTuesday 2023-08-29: Fair Use](https://tidytues.day/2023/2023-08-29)
-   Other places:
    -   Sometimes stricter ([EU](https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:31996L0009:EN:HTML))
    -   Sometimes more lax

::: notes
-   If it's important, consult a lawyer.
-   PII: Or even user-generated information in some cases
-   This is why recipe blogs have stories about the recipes.
-   That TidyTuesday dataset exists because I was writing these notes.
-   EU provides more protections for database creators and for user-generated data
:::

## Should I scrape this data?

`robots.txt`: [github](https://github.com/robots.txt), [wikipedia](https://en.wikipedia.org/robots.txt)

-   `User-agent: *` = everybody
-   Search for name(s) of package(s) 
-   Search for specific pages
-   Check root of site (`/`) and your particular subfolder 
-   These aren't (necessarily) legally binding
-   [{robotstxt}](https://docs.ropensci.org/robotstxt/) üì¶ for parsing `robots.txt`

::: notes
-   No official standards, but certain things are common.
-   Both examples point to the API. This is the right way to write a robots.txt!
-   `robots.txt` might be OVER protective, but watch for notes on how to use it
-   Don't hit a site you like a billion times in fast succession, you could "use up" their monthly bandwidth, for example
-   Your actions COULD cost them money, so don't be a jerk!
-   We'll see the {polite} package shortly, which wraps {robotstxt}
:::

# Introduction to {rvest}

## Three steps of web scraping

1.  Load the page.
2.  Find the object(s) you want.
3.  Extract text into R object(s).

::: notes
-   Pages come in as text, regardless of what they contain.
    -   Some text might point to an image's URL, but the page is just text.
-   Maybe you want a specific table, or specific blocks of information (like a list of products).
-   Again, even if you're getting numbers, they're text until you say otherwise.
-   (put this bullet later): Can use {readr} to `parse_*()` data (or `{stbl}` or probably others)
:::

## Load the page

```{r}
#| label: rvest-read_html
rvest::read_html(
    x,                                    # Usually URL
    encoding = "",                        # Assumes UTF-8, specify if garbled
    ...,                                  # Never used as far as I can find
    options= c(                           # Options for `libxml2` parsers
      "RECOVER", "NOERROR", "NOBLANKS"
    )
)
```

::: notes
-   You'll almost always just send a URL to this function, but it CAN be HTML as text, or a file, or several other things.
-   I don't think I've ever used any options other than defaults.
:::

## Find the object

-   HTML = **H**yper**T**ext **M**arkup **L**anguage
-   Tags as `<tagname attribute="a">contents</tagname>`
-   `rvest::html_element()` output same length as input
-   `rvest::html_elements()` flattens things
-   Can use ["CSS selectors"](https://flukeout.github.io/) or ["XPath expressions"](https://www.w3schools.com/xml/xpath_intro.asp)
    -   We'll focus on CSS selectors

::: notes
-   I often forget the difference between `html_element()` and `html_elements()`
    -   VERY rough rule of thumb: html_elements() once, then html_element to get things inside of those elements.
-   CSS selectors and XPath expressions can each be their own book
    -   W3 schools tutorials are helpful!
    -   CSS diner for CSS selectors
:::

## Extract text

-   TODO: WORKING HERE
-   html_text() vs html_text2()
-   readr::parse_*(), {stbl}?, others?

# Scraping tables

## Example 1: All tables on page

[![The W3 Schools page about HTML Tables](rvest/table_simple.png){height=500}](https://www.w3schools.com/html/html_tables.asp)

::: notes
-   I'll host examples for everything in this chapter in the final book, so I have control over them.
-   Notice that the first row is headers in this case.
-   On the page, scroll down to "HTML Table Tags" for another example of a table.
:::

## Scraping tables

```{r}
#| label: rvest-tables-simple
rvest::html_table(
    x,                  # A page loaded 
    header = NA,
    trim = TRUE,
    dec = ".",
    na.strings = "NA",
    convert = TRUE
)
```

## Example 2: Choose a specific table

(screenshot of tables on page)

::: notes
-   TidyTuesday README has two tables, even though it isn't necessarily obvious (first is the "main" GH content)
:::

## Choosing a table

(code demo of scraping one of many tables)

## Scraping multiple tables

(purrr)

# Scraping non-tabular data

## Example 3: Non-tabular data

(screenshot of page with non-tabular data, possibly CSS selector rules)

## Using SelectorGadget

(record clicks? also show code where it goes)

## Using CSS selectors

# Scraping interactive web pages

## Example 4: Login

## Scraping data behind a login

## Example 5: Dynamic data

## Scraping data that requires interaction
