---
engine: knitr
title: Scrape data from web pages 
---

# Ô∏è‚úÖ Learning objectives

::: nonincremental
-   Decide whether to scrape data from a web page.
-   Use {polite} to responsibly scrape web pages.
-   Scrape complex data structures from web pages.
-   Scrape data from websites that require you to log in.
-   Scrape content that requires interaction.
:::

```{r}
#| label: rvest-packages-used
#| eval: true
library(polite)
library(rvest)
library(xml2)
library(chromote)
```

::: notes
-   "Scraping" a web page means programmatically processing the web page to extract data in a usable format.
    -   Silly personal pet peeve: "scraping" vs "scrapping".
-   Can be a single page or several.
-   Point out that everything after the very beginning is the same as parsing HTML responses from APIs, because web pages are just GET requests
:::

# Decide whether to scrape data

## Do I need to scrape this data?

-   Try {[datapasta](https://cran.r-project.org/package=datapasta)} üì¶
    -   RStudio Addins
-   If it's one time & over-complicated, consider other copy/paste strategies
-   Only scrape what you need
-   Look for an API!

::: notes
-   Even consider manual copy/paste one field at a time if it's not a TON of data.
-   We'll see one place to look for an API on the next slide.
-   More on looking for APIs in chapter 9 "Find APIs"
:::

## Can I legally scrape this data?

-   ‚úÖ Personal use or nonprofit education usually ok
-   ‚ö†Ô∏è Personally Identifiable Information (PII)
-   ‚ö†Ô∏è Legal disclaimers (but may be over-protective)
-   USA:
    -   Can't copyright facts,
    -   *CAN* copyright collections of facts in some cases (creative)
    -   [TidyTuesday 2023-08-29: Fair Use](https://tidytues.day/2023/2023-08-29)
-   Other places:
    -   Sometimes stricter ([EU](https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:31996L0009:EN:HTML))
    -   Sometimes more lax

::: notes
-   If it's important, consult a lawyer.
-   PII: Or even user-generated information in some cases
-   This is why recipe blogs have stories about the recipes.
-   That TidyTuesday dataset exists because I was writing these notes.
-   EU provides more protections for database creators and for user-generated data
:::

## Should I scrape this data?

`robots.txt`: [github](https://github.com/robots.txt), [wikipedia](https://en.wikipedia.org/robots.txt)

-   `User-agent: *` = everybody
-   Search for name(s) of package(s) 
-   Search for specific pages
-   Check root of site (`/`) and your particular subfolder 
-   These aren't (necessarily) legally binding
-   [{robotstxt}](https://docs.ropensci.org/robotstxt/) üì¶ for parsing `robots.txt`
-   [{polite}](https://dmi3kno.github.io/polite/) üì¶ wraps and applies {robotstxt}

::: notes
-   No official standards, but certain things are common.
-   Both examples point to the API. This is the right way to write a robots.txt!
-   `robots.txt` might be OVER protective, but watch for notes on how to use it
-   Don't hit a site you like a billion times in fast succession, you could "use up" their monthly bandwidth, for example
-   Your actions COULD cost them money, so don't be a jerk!
-   We'll see the {polite} package shortly, which wraps {robotstxt}
:::

# Scrape non-tabular data

## Motivating example: Cheese

[![Castelmagno cheese from cheese.com](rvest/castelmagno_cheese.png)](https://www.cheese.com/castelmagno/)

::: notes
-   For the book, I'll host some data so we can be sure it won't change, but I'm using this example because it's roughly the shape I'm looking for.
-   "Coss-tell-man-yo"
:::

# Three steps of web scraping with {rvest}

See [R4DS Chapter 24: "Web Scraping"](https://r4ds.hadley.nz/webscraping) for a full introduction

1.  Load the page.
2.  Find the object(s) (observations & variables) you want.
3.  Extract variables (text) into R object(s).

::: notes
-   Pages = text, regardless of what they contain.
    -   Text might point to URL of image, but page = text.
-   Specific table? Specific blocks of info? 
    -   List of products
-   Even numbers = text until parsed.
-   (put this bullet later): Can use {readr} to `parse_*()` data (or `{stbl}` or probably others)
:::

## Use {polite} to scrape respectfully

::: fragment
```{r}
#| label: rvest-polite-bow
polite::bow(
  url,
  user_agent = "polite R package", # If you don't change this, min delay = 5
  delay = 5,                       # Seconds
  times = 3,                       # Retries
  force = FALSE,                   # Clears memoised functions
  verbose = FALSE,                 # Useful to know why it failed
  ...
)
```
:::
::: fragment
```{r}
#| label: rvest-polite-scrape
polite::scrape(
  bow,
  query = NULL,     # Named list to add after `?` in URL
  accept = "html",  # Specify html, json, xml, csv, txt, etc
  content = NULL,   # Optional MIME type
  verbose = FALSE   # Useful to know why it failed
)
```
:::

::: notes
-   Polite is a great package, but docs are light.
    -   I'm resisting urge to rewrite it in httr2 with better documentation.
-   bow():
    -   Good idea to set a `user agent`. If you leave default, it *has* to be slow.
    -   I'd set `delay` to 0; user agent requirements will override this.
    -   Ok to leave `times` as-is, probably.
    -   `force`: "Memoise" = if inputs are the same, use the previous result.
        -   Ie, don't hit the API again if we already have the result.
    -   I'd recommend setting `verbose` to TRUE unless you're doing a mostly safe, automatic scrape.
-   scrape()
    -   `query` is useful for updating target in a loop, we'll see that later
    -   `accept`: For scraping, you likely want HTML
    -   Leave `content` NULL probably
    -   Set `verbose` to TRUE while testing.
-   We'll see a third function later: `nod()`
:::

## Load the page: bow() + scrape()

::: fragment
```{r}
#| label: rvest-polite-bow-usage
session <- polite::bow(
  "https://www.cheese.com/castelmagno/",
  user_agent = "rvest/1.0.4 (Jon Harmon; mailto:jonthegeek+useragent@gmail.com)",
  delay = 0,
  verbose = TRUE
)
session
#> <polite session> https://www.cheese.com/castelmagno/
#>     User-agent: rvest/1.0.4 (Jon Harmon; https://wapir.io; mailto:jonthegeek+useragent@gmail.com)
#>     robots.txt: 0 rules are defined for 1 bots
#>    Crawl delay: 0 sec
#>   The path is scrapable for this user-agent
```
:::
::: fragment
```{r}
#| label: rvest-polite-scrape-usage
castelmagno_page <- polite::scrape(session)
```
:::

# Working here

` castelmagno_page |> rvest::html_elements(".summary-points")`

-   Link to that page again, use inspect element to find waht we want
-   Reiterate starting with the container ("observation")

## Load the page

```{r}
#| label: rvest-read_html
rvest::read_html(
    x,                                    # Usually URL
    encoding = "",                        # Assumes UTF-8, specify if garbled
    ...,                                  # Never used as far as I can find
    options = c(                          # Options for `libxml2` parsers
      "RECOVER", "NOERROR", "NOBLANKS"    # Many more options than shown
    )
)
```

::: notes
-   x usually = URL, but can be literal HTML text, file, other things
-   ~Always defaults for everything but x
-   {xml2} under the hood
-   Called by httr2::resp_body_html()
:::

## Find the object

-   HTML = **H**yper**T**ext **M**arkup **L**anguage
-   Tags as `<tagname attribute="a">contents</tagname>`
-   `rvest::html_elements()` to select *observations* (rows)
-   `rvest::html_element()` to select *variables* (columns)
-   Can use ["CSS selectors"](https://flukeout.github.io/) or ["XPath expressions"](https://www.w3schools.com/xml/xpath_intro.asp)
    -   We'll focus on CSS selectors

::: notes
-   I often forget the difference between `html_element()` and `html_elements()`
    -   Rule of thumb: html_elements() once, then html_element() to get each variable inside of those elements.
-   CSS selectors and XPath expressions can each be their own book
    -   W3 schools tutorials are helpful!
    -   CSS diner for CSS selectors
    -   I recommend learning the CSS side, because it's also (very) useful for styling docs and shiny apps
:::

## Extract text

-   TODO: WORKING HERE
-   html_text() vs html_text2() vs html_attrs() vs html_attr()
-   readr::parse_*(), {stbl}?, others?

# Scrape non-tabular data (old)

## Example 3: Non-tabular data

::: notes
-   I'll host examples for everything in this chapter in the final book, so I have control over them.
:::

(screenshot of page with non-tabular data, possibly CSS selector rules)

## Using CSS selectors

# Scrape interactive web pages

## Example 4: Login

## Scraping data behind a login

## Example 5: Dynamic data

## Scraping data that requires interaction
